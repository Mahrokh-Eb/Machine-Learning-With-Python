{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cifar10 \n",
    "### loading, training, testing, accuracy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_cifar10_batch(filename):\n",
    "    \"\"\" Load a single batch from CIFAR10 \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype('float')\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "    \n",
    "def load_cifar10(dir):\n",
    "    \"\"\" Load all batches of CIFAR10 \"\"\"\n",
    "    # load train batch file\n",
    "    xs = []\n",
    "    ys = []\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        filename = os.path.join(dir, 'data_batch_%d' % i)\n",
    "        X, Y = load_cifar10_batch(filename)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "        \n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    \n",
    "    # load test batch\n",
    "    Xte, Yte = load_cifar10_batch(os.path.join(dir, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train, X_test, y_test = load_cifar10('/Users/mahrokh/Desktop/UCLA/class-training/cifar-10-batches-py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= (50000, 32, 32, 3)\n",
      "reshaped_xtrain.shape=  (50000, 3072)\n",
      "y_train.shape=  (50000,)\n",
      "x_test=  (10000, 32, 32, 3)\n",
      "reshaped_xtest.shape=  (10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape=', X_train.shape)\n",
    "reshaped_xtrain = X_train.reshape(50000, 3072)\n",
    "print('reshaped_xtrain.shape= ',reshaped_xtrain.shape)\n",
    "print('y_train.shape= ',y_train.shape)\n",
    "print('x_test= ',X_test.shape)\n",
    "reshaped_xtest = X_test.reshape(10000, 3072)\n",
    "print('reshaped_xtest.shape= ',reshaped_xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100,100), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = 2.07744222\n",
      "Iteration 3, loss = 1.98409161\n",
      "Iteration 4, loss = 1.93871360\n",
      "Iteration 5, loss = 1.92372929\n",
      "Iteration 6, loss = 1.87469131\n",
      "Iteration 7, loss = 1.88974602\n",
      "Iteration 8, loss = 1.87976432\n",
      "Iteration 9, loss = 1.85672074\n",
      "Iteration 10, loss = 1.84421486\n",
      "Iteration 11, loss = 1.83891216\n",
      "Iteration 12, loss = 1.81988784\n",
      "Iteration 13, loss = 1.80175335\n",
      "Iteration 14, loss = 1.79153173\n",
      "Iteration 15, loss = 1.77345186\n",
      "Iteration 16, loss = 1.76195373\n",
      "Iteration 17, loss = 1.76205628\n",
      "Iteration 18, loss = 1.72845720\n",
      "Iteration 19, loss = 1.75892815\n",
      "Iteration 20, loss = 1.74781514\n",
      "Iteration 21, loss = 1.73814977\n",
      "Iteration 22, loss = 1.72205636\n",
      "Iteration 23, loss = 1.69929211\n",
      "Iteration 24, loss = 1.68191088\n",
      "Iteration 25, loss = 1.67886503\n",
      "Iteration 26, loss = 1.68236519\n",
      "Iteration 27, loss = 1.68175577\n",
      "Iteration 28, loss = 1.67329961\n",
      "Iteration 29, loss = 1.66183004\n",
      "Iteration 30, loss = 1.64001263\n",
      "Iteration 31, loss = 1.64676827\n",
      "Iteration 32, loss = 1.64098038\n",
      "Iteration 33, loss = 1.63879451\n",
      "Iteration 34, loss = 1.64461187\n",
      "Iteration 35, loss = 1.64036852\n",
      "Iteration 36, loss = 1.63035614\n",
      "Iteration 37, loss = 1.62988813\n",
      "Iteration 38, loss = 1.63070704\n",
      "Iteration 39, loss = 1.61820426\n",
      "Iteration 40, loss = 1.61188142\n",
      "Iteration 41, loss = 1.62439008\n",
      "Iteration 42, loss = 1.61758526\n",
      "Iteration 43, loss = 1.61260980\n",
      "Iteration 44, loss = 1.60584107\n",
      "Iteration 45, loss = 1.61411741\n",
      "Iteration 46, loss = 1.59619922\n",
      "Iteration 47, loss = 1.60497048\n",
      "Iteration 48, loss = 1.59838563\n",
      "Iteration 49, loss = 1.60202897\n",
      "Iteration 50, loss = 1.58904157\n",
      "Iteration 51, loss = 1.58716489\n",
      "Iteration 52, loss = 1.60027174\n",
      "Iteration 53, loss = 1.59220471\n",
      "Iteration 54, loss = 1.58481832\n",
      "Iteration 55, loss = 1.58482951\n",
      "Iteration 56, loss = 1.57990463\n",
      "Iteration 57, loss = 1.56920803\n",
      "Iteration 58, loss = 1.57333684\n",
      "Iteration 59, loss = 1.57160550\n",
      "Iteration 60, loss = 1.56877913\n",
      "Iteration 61, loss = 1.56261528\n",
      "Iteration 62, loss = 1.56124233\n",
      "Iteration 63, loss = 1.55705989\n",
      "Iteration 64, loss = 1.56074828\n",
      "Iteration 65, loss = 1.55241273\n",
      "Iteration 66, loss = 1.55374661\n",
      "Iteration 67, loss = 1.54819284\n",
      "Iteration 68, loss = 1.55357970\n",
      "Iteration 69, loss = 1.54522008\n",
      "Iteration 70, loss = 1.53975190\n",
      "Iteration 71, loss = 1.54079650\n",
      "Iteration 72, loss = 1.53790742\n",
      "Iteration 73, loss = 1.53096707\n",
      "Iteration 74, loss = 1.53390978\n",
      "Iteration 75, loss = 1.53558784\n",
      "Iteration 76, loss = 1.53290496\n",
      "Iteration 77, loss = 1.53320699\n",
      "Iteration 78, loss = 1.52596737\n",
      "Iteration 79, loss = 1.52358993\n",
      "Iteration 80, loss = 1.52143986\n",
      "Iteration 81, loss = 1.52333059\n",
      "Iteration 82, loss = 1.51760138\n",
      "Iteration 83, loss = 1.52070585\n",
      "Iteration 84, loss = 1.51187450\n",
      "Iteration 85, loss = 1.51067195\n",
      "Iteration 86, loss = 1.51937423\n",
      "Iteration 87, loss = 1.52060716\n",
      "Iteration 88, loss = 1.50265652\n",
      "Iteration 89, loss = 1.51167647\n",
      "Iteration 90, loss = 1.50539643\n",
      "Iteration 91, loss = 1.50650331\n",
      "Iteration 92, loss = 1.51034161\n",
      "Iteration 93, loss = 1.50042697\n",
      "Iteration 94, loss = 1.49396444\n",
      "Iteration 95, loss = 1.50435345\n",
      "Iteration 96, loss = 1.49431196\n",
      "Iteration 97, loss = 1.50425797\n",
      "Iteration 98, loss = 1.50955345\n",
      "Iteration 99, loss = 1.49652281\n",
      "Iteration 100, loss = 1.48855257\n",
      "Iteration 101, loss = 1.49116835\n",
      "Iteration 102, loss = 1.48978881\n",
      "Iteration 103, loss = 1.48891617\n",
      "Iteration 104, loss = 1.48078539\n",
      "Iteration 105, loss = 1.49807708\n",
      "Iteration 106, loss = 1.48871962\n",
      "Iteration 107, loss = 1.47495604\n",
      "Iteration 108, loss = 1.48632778\n",
      "Iteration 109, loss = 1.48485649\n",
      "Iteration 110, loss = 1.49140247\n",
      "Iteration 111, loss = 1.46765747\n",
      "Iteration 112, loss = 1.49051267\n",
      "Iteration 113, loss = 1.47783789\n",
      "Iteration 114, loss = 1.47709097\n",
      "Iteration 115, loss = 1.46856450\n",
      "Iteration 116, loss = 1.47684847\n",
      "Iteration 117, loss = 1.46937571\n",
      "Iteration 118, loss = 1.47304843\n",
      "Iteration 119, loss = 1.46889228\n",
      "Iteration 120, loss = 1.46657033\n",
      "Iteration 121, loss = 1.46737115\n",
      "Iteration 122, loss = 1.47253743\n",
      "Iteration 123, loss = 1.47459974\n",
      "Iteration 124, loss = 1.46988415\n",
      "Iteration 125, loss = 1.45922993\n",
      "Iteration 126, loss = 1.46832968\n",
      "Iteration 127, loss = 1.46355195\n",
      "Iteration 128, loss = 1.46715736\n",
      "Iteration 129, loss = 1.45020697\n",
      "Iteration 130, loss = 1.45336968\n",
      "Iteration 131, loss = 1.46331307\n",
      "Iteration 132, loss = 1.46779860\n",
      "Iteration 133, loss = 1.46632881\n",
      "Iteration 134, loss = 1.46244472\n",
      "Iteration 135, loss = 1.46918135\n",
      "Iteration 136, loss = 1.45630904\n",
      "Iteration 137, loss = 1.45238494\n",
      "Iteration 138, loss = 1.45635295\n",
      "Iteration 139, loss = 1.45469268\n",
      "Iteration 140, loss = 1.45200152\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 100, 100, 100), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(reshaped_xtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(reshaped_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3618\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
