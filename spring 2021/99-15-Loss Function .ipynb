{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impaired-uncle",
   "metadata": {},
   "source": [
    "# SVM Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-price",
   "metadata": {},
   "source": [
    "### unvectorized implementation - näive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-cyprus",
   "metadata": {},
   "source": [
    "def l_i(x, y, w):\n",
    "    scores = w.dot(x)\n",
    "    correct_class_score = scores[y]\n",
    "    c =w.shape[0]\n",
    "    \n",
    "    loss_i = 0.0\n",
    "    for j in range(c):\n",
    "        if j==c:\n",
    "            continue\n",
    "        loss_i += max(0, scores[j] - correct_class_score +1.0)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-tenant",
   "metadata": {},
   "source": [
    "### Half_vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flush-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def l_i_half_vectorized(x, y, w):\n",
    "    scores = w.dot(x)\n",
    "    margins = np.maximum(0, scores - scores[y] + 1.0)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-lexington",
   "metadata": {},
   "source": [
    "### Fully_vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pretty-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_i_fully_vectorised(x, y, w):\n",
    "    N = scores.shape[0]\n",
    "    scores = w.dot(x)\n",
    "    correct_class_score = scores[range(N), y]\n",
    "    margins = np.maximum(0.0, scores - correct_class_score[:,None] + 1.0)\n",
    "    margins[range(N), y] = 0.0\n",
    "    loss = np.sum(margins) / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-syracuse",
   "metadata": {},
   "source": [
    "# Softmax Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-footage",
   "metadata": {},
   "source": [
    "### unvectorized implementation - näive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "municipal-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(scores, y, W, reg=1e-3):\n",
    "    \n",
    "    N, C = scores.shape\n",
    "\n",
    "    # compute data loss\n",
    "    loss = 0.0\n",
    "    for i in range(N):\n",
    "        correct_class = y[i]\n",
    "        score = scores[i]\n",
    "        score -= np.max(scores)\n",
    "        exp_score = np.exp(score)\n",
    "        probs = exp_score / np.sum(exp_score)\n",
    "        loss += -np.log(probs[correct_class])\n",
    "\n",
    "    loss /= N\n",
    "\n",
    "    # compute regularization loss\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "\n",
    "    # Compute gradient off loss function w.r.t. scores\n",
    "    # We will write this part later\n",
    "    grads = {}  \n",
    "\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-richards",
   "metadata": {},
   "source": [
    "### Half_vectorized implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "surprising-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(scores, y, W, reg=1e-3):\n",
    "    N = scores.shape[0]  # number of input data\n",
    "\n",
    "    # compute data loss\n",
    "    scores -= np.max(scores, axis=1, keepdims=True)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    loss = -np.sum(np.log(probs[range(N), y])) / N\n",
    "\n",
    "    # compute regularization loss\n",
    "    loss += 0.5 * reg * np.sum(W * W)\n",
    "\n",
    "    # Compute gradient off loss function w.r.t. scores\n",
    "    # We will write this part later\n",
    "    grads = {}  \n",
    "\n",
    "    return loss, grads   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-methodology",
   "metadata": {},
   "source": [
    "### Softmax loss, forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "passive-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(s, y):\n",
    "    \n",
    "    #forward step\n",
    "    shifted_log = s - np.max(s, axis=1, keepdims=True)\n",
    "    z = np.sum(np.exp(shifted_log), axis=1, keepdims=True)\n",
    "    log_probs = shifted_log - np.log(z)\n",
    "    probs = np.exp(log_probs)\n",
    "    N = s.shape[0]\n",
    "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
    "    \n",
    "    #backward step\n",
    "    ds = probs.copy() \n",
    "    ds[np.arange(N), y] -= 1\n",
    "    ds /= N\n",
    "    return loss, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-medicare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
